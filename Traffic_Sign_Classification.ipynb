{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Traffic Sign Classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPM-6I6wb34L"
      },
      "source": [
        "!pip install tensorflow==2.4.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNrrhvOWcEMp"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SBMGKq2HTHb"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense, Conv2D, AvgPool2D, Activation, Flatten, MaxPool2D, Dropout\n",
        "\n",
        "\n",
        "\n",
        "class CNN_networks():\n",
        "    def __init__(self, shape):\n",
        "        super().__init__()\n",
        "        self.height, self.width, self.channel = shape\n",
        "\n",
        "\n",
        "    def LeNet(self):\n",
        "        model = keras.models.Sequential()\n",
        "\n",
        "        model.add(Conv2D(filters=6, kernel_size=5, strides=1,\n",
        "                         input_shape = [self.height, self.width, self.channel],\n",
        "                         padding = \"same\",\n",
        "                         activation='tanh'))\n",
        "        # shape = (32, 32, 6)\n",
        "        model.add(AvgPool2D(pool_size=2, strides = 2))\n",
        "        model.add(Activation('tanh'))\n",
        "        # shape = (16, 16, 6)\n",
        "\n",
        "        model.add(Conv2D(filters=16, kernel_size=5, strides=1,\n",
        "                         activation='tanh'))\n",
        "        # shape = (12, 12, 16)\n",
        "        model.add(AvgPool2D(pool_size=2, strides = 2))\n",
        "        model.add(Activation('tanh'))\n",
        "        # shape = (6, 6, 16)\n",
        "\n",
        "        model.add(Conv2D(filters=120, kernel_size = 5, \n",
        "                         strides=1, activation= 'tanh'))\n",
        "        # shape = (2, 2, 120)\n",
        "\n",
        "        # Flatten the input for fully connected layers\n",
        "        model.add(Flatten())\n",
        "\n",
        "        # fully connected layer\n",
        "        model.add(Dense(units = 84, activation='tanh'))\n",
        "\n",
        "        # output layer instead of RBF, I used softmax\n",
        "        model.add(Dense(units=43, activation='softmax'))\n",
        "\n",
        "        return model\n",
        "\n",
        "\n",
        "    def CustomNet(self):\n",
        "        model = keras.models.Sequential()\n",
        "\n",
        "        model.add(Conv2D(filters=32, kernel_size=7, strides=1,\n",
        "                         input_shape = (self.height, self.width, self.channel),\n",
        "                         padding = \"same\",\n",
        "                         activation='relu'))\n",
        "        # shape = (32, 32, 64)\n",
        "        model.add(Conv2D(filters=64, kernel_size=3, strides=1,\n",
        "                         padding = \"same\",\n",
        "                         activation='relu'))\n",
        "        # shape = (32, 32, 128)\n",
        "        model.add(MaxPool2D(pool_size=2, strides = 1))\n",
        "        # shape = (31, 31, 128)\n",
        "\n",
        "        # Flatten the input for fully connected layers\n",
        "        model.add(Flatten())\n",
        "\n",
        "        # fully connected layer 2\n",
        "        model.add(Dense(units=64, activation='relu'))\n",
        "        model.add(Dropout(0.05))\n",
        "\n",
        "        # output layer\n",
        "        model.add(Dense(units=43, activation='softmax'))\n",
        "\n",
        "        #compile\n",
        "        # opt = keras.optimizers.Adam(learning_rate=0.02)\n",
        "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "        return model\n",
        "\n",
        "\n",
        "    def GoogleNet(self):\n",
        "        pass\n",
        "\n",
        "# shape = (5, 32, 32, 3)\n",
        "# n = CNN_networks(shape[1:])\n",
        "# model = n.CustomNet()\n",
        "# x =  tf.random.normal(shape)\n",
        "# # x = np.random.uniform(shape)\n",
        "# y = np.array([4, 2, 3, 1, 0])\n",
        "# y = keras.utils.to_categorical(y, num_classes=43)\n",
        "# print(x.shape)\n",
        "# print(y.shape)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAibWzS1FQBq"
      },
      "source": [
        "# https://www.kaggle.com/valentynsichkar/traffic-signs-classification-with-cnn/data?select=datasets_preparing.py\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle as pkl\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import shuffle\n",
        "from tensorflow import keras\n",
        "\n",
        "\n",
        "\n",
        "def load_data(file):\n",
        "    file_path = '/content/drive/MyDrive/traffic_sign/input/'+str(file)\n",
        "    with open(file_path, 'rb') as f:\n",
        "        d = pkl.load(f, encoding = 'latin1')\n",
        "        x = d['features'].astype(np.float32)\n",
        "        y = d['labels']\n",
        "        c = d['coords']\n",
        "        s = d['sizes']\n",
        "\n",
        "    return x, y, s, c\n",
        "\n",
        "\n",
        "def normalize_data(data):\n",
        "    data = data/255.0\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def labelname_list(file):\n",
        "    file_path = '/content/drive/MyDrive/traffic_sign/input/'+str(file)\n",
        "    labels = pd.read_csv(file_path)\n",
        "    label_list = []\n",
        "    # print(len(labels))\n",
        "    for row in range(len(labels)):\n",
        "        label_list.append(labels['SignName'][row])\n",
        "    \n",
        "    return label_list\n",
        "\n",
        "# load training data\n",
        "x_train, y_train, s_train, c_train = load_data('train.pickle')\n",
        "\n",
        "# load validation data\n",
        "x_valid, y_valid, s_valid, c_valid = load_data('valid.pickle')\n",
        "\n",
        "# load test data\n",
        "x_test, y_test, s_test, c_test = load_data('test.pickle')\n",
        "\n",
        "# normailize features from train, valid, and test\n",
        "x_train, x_valid, x_test = map(normalize_data, [x_train, x_valid, x_test])\n",
        "\n",
        "# shuffle \n",
        "x_train, y_train = shuffle(x_train, y_train, random_state=0)\n",
        "x_valid, y_valid = shuffle(x_valid, y_valid, random_state=0)\n",
        "x_test, y_test = shuffle(x_test, y_test, random_state=0)\n",
        "\n",
        "# to categorical\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes=43)\n",
        "y_valid = keras.utils.to_categorical(y_valid, num_classes=43)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes=43)\n",
        "\n",
        "# label name list\n",
        "label_list = labelname_list('label_names.csv')\n",
        "\n",
        "def show_image(x, y):\n",
        "    plt.figure()\n",
        "    plt.imshow(x)\n",
        "    plt.title(label_list[np.argmax(y)])\n",
        "    plt.show()\n",
        "\n",
        "def train():\n",
        "  epochs = 10\n",
        "\n",
        "  print(x_train.shape[1:])\n",
        "  net = CNN_networks(x_train.shape[1:])\n",
        "  model = net.CustomNet()\n",
        "  model.summary()\n",
        "  hist = model.fit(x_train, y_train, epochs = epochs,\n",
        "                   validation_data = (x_valid, y_valid),\n",
        "              verbose=1)\n",
        "\n",
        "  print('Epochs={0:d}, training accuracy={1:.5f}, validation accuracy={2:.5f}'.\\\n",
        "        format(epochs, max(hist.history['accuracy']), max(hist.history['val_accuracy'])))\n",
        "\n",
        "\n",
        "  model.save('/content/drive/MyDrive/traffic_sign/input/model3')\n",
        "\n",
        "\n",
        "# print('label_list:', len(label_list))\n",
        "# print(np.mean(x_train, axis=0).shape)\n",
        "# print('x_train, x_valid, x_test', type(x_train), x_valid.shape, x_test.shape)\n",
        "# print('y_train:', y_train[5])\n",
        "# print('s_train:', s_train.shape)\n",
        "# print('c_train:', c_train.shape)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkScrwfkdTbh",
        "outputId": "15d960c7-c5a3-46ba-a74f-3404b15ad66e"
      },
      "source": [
        "train()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 32, 3)\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_22 (Conv2D)           (None, 32, 32, 32)        4736      \n",
            "_________________________________________________________________\n",
            "conv2d_23 (Conv2D)           (None, 32, 32, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_13 (MaxPooling (None, 31, 31, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_7 (Flatten)          (None, 61504)             0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 64)                3936320   \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 43)                2795      \n",
            "=================================================================\n",
            "Total params: 3,962,347\n",
            "Trainable params: 3,962,347\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "1088/1088 [==============================] - 17s 15ms/step - loss: 1.9859 - accuracy: 0.4879 - val_loss: 0.4403 - val_accuracy: 0.8741\n",
            "Epoch 2/10\n",
            "1088/1088 [==============================] - 16s 14ms/step - loss: 0.2362 - accuracy: 0.9323 - val_loss: 0.2885 - val_accuracy: 0.9147\n",
            "Epoch 3/10\n",
            "1088/1088 [==============================] - 16s 14ms/step - loss: 0.1251 - accuracy: 0.9626 - val_loss: 0.2828 - val_accuracy: 0.9286\n",
            "Epoch 4/10\n",
            "1088/1088 [==============================] - 16s 14ms/step - loss: 0.0847 - accuracy: 0.9748 - val_loss: 0.2780 - val_accuracy: 0.9288\n",
            "Epoch 5/10\n",
            "1088/1088 [==============================] - 16s 15ms/step - loss: 0.0579 - accuracy: 0.9823 - val_loss: 0.2322 - val_accuracy: 0.9401\n",
            "Epoch 6/10\n",
            "1088/1088 [==============================] - 16s 14ms/step - loss: 0.0531 - accuracy: 0.9836 - val_loss: 0.2037 - val_accuracy: 0.9492\n",
            "Epoch 7/10\n",
            "1088/1088 [==============================] - 16s 14ms/step - loss: 0.0389 - accuracy: 0.9894 - val_loss: 0.2798 - val_accuracy: 0.9311\n",
            "Epoch 8/10\n",
            "1088/1088 [==============================] - 16s 14ms/step - loss: 0.0383 - accuracy: 0.9876 - val_loss: 0.2055 - val_accuracy: 0.9501\n",
            "Epoch 9/10\n",
            "1088/1088 [==============================] - 16s 14ms/step - loss: 0.0293 - accuracy: 0.9912 - val_loss: 0.2540 - val_accuracy: 0.9392\n",
            "Epoch 10/10\n",
            "1088/1088 [==============================] - 16s 14ms/step - loss: 0.0339 - accuracy: 0.9898 - val_loss: 0.2188 - val_accuracy: 0.9583\n",
            "Epochs=10, training accuracy=0.99083, validation accuracy=0.95828\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/traffic_sign/input/model3/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1WF8vgmFP8b",
        "outputId": "71df2005-1a00-4226-cf68-7c13e1489dec"
      },
      "source": [
        "new_model = tf.keras.models.load_model('/content/drive/MyDrive/traffic_sign/input/model3')\n",
        "res = new_model.predict(x_test[5:6, :, :, :])\n",
        "c = np.argmax(res)\n",
        "print(c)\n",
        "print(label_list[c])\n",
        "# new_model.summary()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "38\n",
            "Keep right\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "teE3sEVvUALd",
        "outputId": "0ade4f21-0f27-4eff-c340-6fb07fed0089"
      },
      "source": [
        "show_image(x_test[5, :, :, :], y_test[5])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQLGWk3_FP50"
      },
      "source": [
        "loss, acc = new_model.evaluate(x_test, y_test, verbose=2)\n",
        "print('Restored model, accuracy: {:5.2f}%'.format(100 * acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGnqRnboFP3l"
      },
      "source": [
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (15.0, 5.0) # Setting default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.plot(hist.history['accuracy'], '-o', linewidth=3.0)\n",
        "plt.plot(hist.history['val_accuracy'], '-o', linewidth=3.0)\n",
        "plt.title('Traffic Sign Classification', fontsize=22)\n",
        "plt.legend(['train', 'validation'], loc='lower right', fontsize='xx-large')\n",
        "plt.xlabel('Epoch', fontsize=15)\n",
        "plt.ylabel('Accuracy', fontsize=15)\n",
        "# plt.tick_params(labelsize=13)\n",
        "\n",
        "# Showing the plot\n",
        "plt.show()\n",
        "\n",
        "# Saving the plot\n",
        "fig.savefig('/content/drive/MyDrive/traffic_sign/figures/training_data.png')\n",
        "plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMwLYwcNFP1E"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}